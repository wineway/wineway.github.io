---
layout:     post
title:      "[笔记]Spark MLlib"
author:     wineway
tags: 		spark 未完成
subtitle:   spark学习笔记
category:  project1
visualworkflow: true
---
## RDD 操作

- `parallelize[T](Seq: Seq[T], numSlices: Int = defaultParallelism)(implicit arg0: ClassTag[T]): RDD[T]`: 集群众每个CPU对应2~4个分片,Spark会根据集群情况自行设定分片数量.
- `textFile(path: String, minPartition:Int = defaultMinPartitions): RDD[String]`: HDFS 默认块为64MB, 默认每个块一个分片, 分片数量不少于文件块数量.
- map,filter,reduce etc.
- `mapPartitions[U: ClassTag](f: Iterator[T] => Iterator[U], preservesPartitioning: Boolean = false): RDD[U]`
- `mapPartitionsWithIndex`: f: (Int,Iterator[T]) => Iterator[U]
- `sample(withReplacement, fraction, seed)`:withReplacement,是否放回抽样,seed,随机数种子,fraction,比例
- `union(otherDataset)`:数据合并
- `intersection(otherDataset)`:数据交集
- `distinct([numTask])`:数据去重,numTask是任务并行数量;
- `groupByKey([numTask])`:(K, V) -> (K, Seq[V])
- `reduceByKey(func, [numTask])`: key 分组 func 聚合
- `aggregateByKey(zeroValue: U)(seqOp: (U, T) => U, combOp: (U, U) => U)`: zeroValue:初始值,seqOp:将T 合并到 U,combOp: 合并两个U
- `combineByKey[C](createCombiner: V => C, mergeValue: (C, V) => C, mergeCombiners: (C, C) => C, numPartitions: Int): RDD[(K, C)]`:createCombiner: 首次便利到K将V转换为C,mergeValue: 再碰到K将V聚合到C,mergeCombiners:最终聚合
- `sortByKey([ascending], [numTask])`: ascending: 默认为 true,升序, 对(K, V) 类型按照K排序, K需要实现 Ordered 方法.
- `join(otherDataset, [numTask])`:将数据集(K, V)和另一个数据集(K, W) 做笛卡尔积.
- `cogroup(otherDataset,[numTask]): RDD[(K, Seq[V],Seq[W])]`
- `cartesian(otherDataset): RDD[(T, U)]`: 笛卡尔积
- `pipe(command, [envVars])`: 以shell命令处理 RDD 数据
- `randomSplit(weights: Array[Double], seed: Long = Utils.random.nextLong): Array[RDD[T]]`: 对RDD按照权重重新随机数据分割.
- `subtract(otherDataset: RDD[T]): RDD[T]`: 减去other中包含元素.
- `zip[U](other: RDD[U])(implicit arg0: ClassTag[U]): RDD[(T,U)]`: 还有 `zipWithIndex`, `zipParititions`.
- `coalesce(numPartitions)` 重新分区,默认不 shuffle
- `repartition` shuffle
- `treeAggregate[U](zeroValue: U)(seqOp: (U, T) => U, combOp: (U, U) => U, depth: Int = 2)`:
- `collect`: 集合中所有元素以数组形式返回;`count`:返回数据集中元素个数;`frist`返回数据集中第一个元素,类似take(1);`take(n)`返回一个包含数据集中前n个元素的数组,当前操作不能并行;`takeSample(withReplacement, num, [seed])` num 为取出个数,和Sample不同为返回数组;`takeOrdered(n, [ordering])`:返回包含随机n个元素的数组,按照顺序输出;`saveAsTextFile` 把数据集中写入到一个文本文件,自动对每个元素调用`toString`方法;`countByKey`:对于(K, V)类型RDD,返回一个(K, Int)的map;`foreach(func)`;

## MLlib Statistics 统计操作

- `val S = Statistics.colStats(RDD[VVector])`
  - `S.max`
  - `S.min`
  - `S.mean`
  - `S.variance`
  - `S.normL1`
  - `S.normL2`
- `val c = Statistics.corr(data, "pearson/spearman")` 相关系数
- `val t = Statistics.chiSqTest(v1,v2)` 卡方检验

## MLlib 数据格式

- `MLUtils` :辅助加载保存处理MLlib相关算法所需要的数据
  - `loadLibSVMFile(sc: SparkContext, path: String, numFeatures: Int, minPartition: Int)` : minPartition: 最小分区数
  - `saveAsLibSVMFile(data: RDD[LabledPiont], dir: String)`
  - `appendBias(vector: Vector): Vector`
  - `fastSquaredDistance(v1: Vector, norm1: Double, v2 :Vector, norm2: Double, precision: Double = 1e-6): Double` :L2
- `generateKMeansRDD(sc: SparkContext, numPoints: Int, k: Int, d: Int, r: Double, numPartitions: Int = 2): RDD[Array[Double]]`: r,初始中心分布缩放因子,也可以按照MLlib封装方法进行调用,格式为:`KMeansGenerator <master> <output_dir> <num_point> <k> <d> <r> [<num_partitions>]`: master,集群master地质
- `generateLinearRDD(sc: Spark, nexamples: Int, nfeatures: Int, eps: Double, npart: Int = 2, intercept: Double = 0.0): RDD[LabledPiont]`: eps, epsilon因子; npart,RDD分区数
- `generateLogisticRDD(sc: SparkContext, nexamples: Int, nfeatures: Int, eps: Double, nparts: Int = 2, probOne: Double = 0.5): RDD[LabeledPoint]`
- `MFDataGenerator <master> <outputDir> [m] [n] [rank] [trainSampFact] [noise] [sigma] [test] [testSampFact]`
- `SVMGenerator <master> <output_dir> [num_examples] [num_features] [num_partitions]`
